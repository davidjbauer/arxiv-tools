{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "# Used some stuff from here: https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "\n",
    "pd.options.display.max_seq_items = 1000\n",
    "request_string = (\"http://export.arxiv.org/api/query\")\n",
    "#arxiv_category_list = ['cat:cond-mat.str-el','cat:cond-mat.mes-hall','cat:cond-mat.dis-nn',\n",
    "#            'cat:cond-mat.stat-mech','cat:cond-mat.supr-con','cat:cond-mat.other','cond-mat.quant-gas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All we need from our arxiv database is the arxiv id XXXX.YYYYY\n",
    "# We need to extract this from the abstract or PDF link\n",
    "our_preprints = pd.read_csv(\"roy-group-arxiv-8-2-18.csv\")\n",
    "our_preprints_clean = pd.DataFrame(our_preprints.link.str.split('/').map(lambda x: x[-1][:10]))\n",
    "our_preprints_clean['in_db'] = 1\n",
    "our_preprints_clean.columns=['id','in_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_preprints = pd.read_csv(\"arxiv-since-10-18-17.csv\",\n",
    "                             usecols=['link','title','summary'])\n",
    "arxiv_preprints.columns = ['title','id','abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_preprints['id']=arxiv_preprints.id.str.split('/').map(lambda x: x[-1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preprints = arxiv_preprints.merge(our_preprints_clean, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_preprints = all_preprints.fillna(0)\n",
    "abs_train,abs_test,y_train,y_test=train_test_split(all_preprints.abstract,all_preprints.in_db, test_size=0.33, random_state=28008135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('svm-clf', SGDClassifier(loss='hinge', penalty='l2',max_iter=5))])\n",
    "text_clf_nb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('nb-clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ar_tf=False, use_idf=True)), ('nb-clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_nb.fit(abs_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2),(1,3)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'nb-clf__alpha': (1,1e-1,1e-2, 1e-3,1e-4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf_nb, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(abs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb-clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235913726801538"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
