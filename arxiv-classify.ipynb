{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# Used some stuff from here: https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "pd.options.display.max_seq_items = 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat,max_result in number_per_category.items():\n",
    "    query_params={'search_query':cat,'sortBy':'lastUpdatedDate',\n",
    "                  'sortOrder':'descending',\n",
    "                  'max_results':max_result,\n",
    "                  'start':0}\n",
    "    req = requests.post(request_string,data=query_params)\n",
    "    req.raise_for_status()\n",
    "    \n",
    "    entry_dict = feedparser.parse(req.text)['entries']\n",
    "    \n",
    "    full_df = pd.DataFrame(entry_dict)\n",
    "    \n",
    "    # author list stored as an annoying dict, so unpack it to a comma-separated list.\n",
    "    temp_df=pd.DataFrame(entry_dict,columns=['authors','title','updated','link','summary'])\n",
    "    temp_df['authors']=[', '.join([n['name'] for n in entry]) for entry in list(temp_df['authors'].values)]\n",
    "    \n",
    "    preprint_df = preprint_df.append(temp_df)\n",
    "\n",
    "preprint_df = preprint_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All we need from our arxiv database is the arxiv id XXXX.YYYYY\n",
    "# We need to extract this from the abstract or PDF link\n",
    "our_preprints = pd.read_csv(\"roy-group-arxiv-8-2-18.csv\")\n",
    "our_preprints_clean = pd.DataFrame(our_preprints.link.str.split('/').map(lambda x: x[-1][:10]))\n",
    "our_preprints_clean['in_db'] = 1\n",
    "our_preprints_clean.columns=['id','in_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arxiv_category_list = ['cat:cond-mat.str-el','cat:cond-mat.mes-hall','cat:cond-mat.dis-nn',\n",
    "#            'cat:cond-mat.stat-mech','cat:cond-mat.supr-con','cat:cond-mat.other','cond-mat.quant-gas']\n",
    "request_string = (\"http://export.arxiv.org/api/query\")\n",
    "\n",
    "# We want to get all of the possible relevant articles from cond-mat going back\n",
    "# to the start of our lit database.\n",
    "# I queried each category separately (by trial and error) to find how many articles \n",
    "# were posted since we started our database.\n",
    "\n",
    "#number_per_category = {'cat:cond-mat.str-el':3500,\n",
    "#                       'cat:cond-mat.mes-hall':5000,\n",
    "#                       'cat:cond-mat.dis-nn':950,\n",
    "#                       'cat:cond-mat.stat-mech':3150,\n",
    "#                       'cat:cond-mat.supr-con':1600,\n",
    "#                       'cat:cond-mat.other':420,\n",
    "#                       'cat:cond-mat.quant-gas':1500}\n",
    "number_per_category = {}\n",
    "preprint_df = pd.DataFrame()\n",
    "full_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_preprints = pd.read_csv(\"arxiv-since-10-18-17.csv\",\n",
    "                             usecols=['link','title','summary'])\n",
    "arxiv_preprints.columns = ['title','id','abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_preprints['id']=arxiv_preprints.id.str.split('/').map(lambda x: x[-1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preprints = arxiv_preprints.merge(our_preprints_clean, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_preprints = all_preprints.fillna(0)\n",
    "abs_train,abs_test,y_train,y_test=train_test_split(all_preprints.abstract,all_preprints.in_db, test_size=0.33, random_state=8008135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('svm-clf', SGDClassifier(loss='hinge', penalty='l2',max_iter=5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(abs_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_prediction=text_clf.predict(abs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9253224711473184"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(svm_prediction==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term = count_vec.fit_transform(all_preprints.abstract)\n",
    "tokens = count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(doc_term)]\n",
    "df = pd.DataFrame(data=doc_term.toarray(), index=doc_names,\n",
    "                      columns=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['00', '000', '00000', '00005', '0001', '00013', '00014', '00082',\n",
      "       '00089', '001',\n",
      "       ...\n",
      "       'zrte5', 'zt', 'ztp', 'zumino', 'zureck', 'zurek', 'zv', 'zwanzig',\n",
      "       'zy', 'zz'],\n",
      "      dtype='object', length=24106)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
